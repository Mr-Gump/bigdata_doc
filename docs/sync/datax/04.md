# 第4章 DataX使用

## 4.1 DataX使用概述

### 4.1.1 DataX任务提交命令

DataX 的使用十分简单，用户只需根据自己同步数据的数据源和目的地选择相应的 Reader 和 Writer，并将 Reader 和 Writer 的信息配置在一个 json 文件中，然后执行如下命令提交数据同步任务即可。

```shell
python bin/datax.py path/to/your/job.json
```

### 4.2.2 DataX配置文件格式

可以使用如下命名查看 DataX 配置文件模板。

```shell
 python bin/datax.py -r mysqlreader -w hdfswriter
```

配置文件模板如下，json 最外层是一个 job，job 包含 setting 和 content 两部分，其中 setting 用于对整个 job 进行配置，content 用户配置数据源和目的地。

![image-20230306104626791](https://cos.gump.cloud/uPic/image-20230306104626791.png)

Reader 和 Writer 的具体参数可参考[:link:官方文档](https://github.com/alibaba/DataX/blob/master/README.md )

![image-20230306104956915](https://cos.gump.cloud/uPic/image-20230306104956915.png)

## 4.2 同步MySQL数据到HDFS案例

**案例要求**：同步 gmall 数据库中 base_province 表数据到 HDFS 的 /base_province 目录

**需求分析**：要实现该功能，需选用 MySQLReader 和 HDFSWriter，MySQLReader 具有两种模式分别是 TableMode 和  QuerySQLMode，前者使用 table，column，where 等属性声明需要同步的数据；后者使用一条 SQL 查询语句声明需要同步的数据。需求分析：要实现该功能，需选用 MySQLReader 和 HDFSWriter，MySQLReader 具有两种模式分别是 TableMode 和 QuerySQLMode，前者使用 table，column，where 等属性声明需要同步的数据；后者使用一条 SQL 查询语句声明需要同步的数据。

下面分别使用两种模式进行演示。

### 4.2.1 MySQLReader之TableMode

1）编写配置文件

（1）创建配置文件 base_province.json

```shell
vim /opt/module/datax/job/base_province.json
```

（2）配置文件内容如下

```json title="base_province.json"
{
    "job": {
        "content": [
            {
                "reader": {
                    "name": "mysqlreader",
                    "parameter": {
                        "column": [
                            "id",
                            "name",
                            "region_id",
                            "area_code",
                            "iso_code",
                            "iso_3166_2"
                        ],
                        "where": "id>=3",
                        "connection": [
                            {
                                "jdbcUrl": [
                                    "jdbc:mysql://hadoop102:3306/gmall"
                                ],
                                "table": [
                                    "base_province"
                                ]
                            }
                        ],
                        "password": "000000",
                        "splitPk": "",
                        "username": "root"
                    }
                },
                "writer": {
                    "name": "hdfswriter",
                    "parameter": {
                        "column": [
                            {
                                "name": "id",
                                "type": "bigint"
                            },
                            {
                                "name": "name",
                                "type": "string"
                            },
                            {
                                "name": "region_id",
                                "type": "string"
                            },
                            {
                                "name": "area_code",
                                "type": "string"
                            },
                            {
                                "name": "iso_code",
                                "type": "string"
                            },
                            {
                                "name": "iso_3166_2",
                                "type": "string"
                            }
                        ],
                        "compress": "gzip",
                        "defaultFS": "hdfs://hadoop102:8020",
                        "fieldDelimiter": "\t",
                        "fileName": "base_province",
                        "fileType": "text",
                        "path": "/base_province",
                        "writeMode": "append"
                    }
                }
            }
        ],
        "setting": {
            "speed": {
                "channel": 1
            }
        }
    }
}
```

2）配置文件说明

（1）Reader 参数说明

![image-20230306110127045](https://cos.gump.cloud/uPic/image-20230306110127045.png)

（2）Writer 参数说明

![image-20230306110148687](https://cos.gump.cloud/uPic/image-20230306110148687.png)

!!! warning "注意事项"

    HFDS Writer 并未提供 nullFormat 参数：也就是用户并不能自定义 null 值写到 HFDS 文件中的存储格式。默认情况下，HFDS Writer 会将 null 值存储为空字符串（''），而 Hive 默认的 null 值存储格式为 \N。所以后期将 DataX 同步的文件导入 Hive 表就会出现问题。
    
    解决该问题的方案有两个：
    
    一是修改 DataX HDFS Writer 的源码，增加自定义 null 值存储格式的逻辑，可参考[:link:博客](https://blog.csdn.net/u010834071/article/details/105506580)。
    
    二是在 Hive 中建表时指定 null 值存储格式为空字符串（''），例如：
    
    ```sql
    DROP TABLE IF EXISTS base_province;
    CREATE EXTERNAL TABLE base_province
    (
        `id`         STRING COMMENT '编号',
        `name`       STRING COMMENT '省份名称',
        `region_id`  STRING COMMENT '地区ID',
        `area_code`  STRING COMMENT '地区编码',
        `iso_code`   STRING COMMENT '旧版ISO-3166-2编码，供可视化使用',
        `iso_3166_2` STRING COMMENT '新版IOS-3166-2编码，供可视化使用'
    ) COMMENT '省份表'
        ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
        NULL DEFINED AS ''
        LOCATION '/base_province/';
    ```

（3）Setting 参数说明

![image-20230306110959855](https://cos.gump.cloud/uPic/image-20230306110959855.png)

3）提交任务

（1）在 HDFS 创建 /base_province 目录

使用 DataX 向 HDFS 同步数据时，需确保目标路径{==已存在==}

```shell
hadoop fs -mkdir /base_province
```

（2）进入 DataX 根目录

```shell
cd /opt/module/datax 
```

（3）执行如下命令

```shell
python bin/datax.py job/base_province.json
```

4）查看结果

（1）DataX 打印日志

```shell
2021-10-13 11:13:14.930 [job-0] INFO  JobContainer - 
任务启动时刻                    : 2021-10-13 11:13:03
任务结束时刻                    : 2021-10-13 11:13:14
任务总计耗时                    :                 11s
任务平均流量                    :               66B/s
记录写入速度                    :              3rec/s
读出记录总数                    :                  32
读写失败总数                    :                   0
```

（2）查看 HDFS 文件

```shell
hadoop fs -cat /base_province/* | zcat

3	山西	1	140000	CN-14	CN-SX
4	内蒙古	1	150000	CN-15	CN-NM
5	河北	1	130000	CN-13	CN-HE
6	上海	2	310000	CN-31	CN-SH
7	江苏	2	320000	CN-32	CN-JS
8	浙江	2	330000	CN-33	CN-ZJ
9	安徽	2	340000	CN-34	CN-AH
10	福建	2	350000	CN-35	CN-FJ
11	江西	2	360000	CN-36	CN-JX
12	山东	2	370000	CN-37	CN-SD
14	台湾	2	710000	CN-71	CN-TW
15	黑龙江	3	230000	CN-23	CN-HL
16	吉林	3	220000	CN-22	CN-JL
17	辽宁	3	210000	CN-21	CN-LN
18	陕西	7	610000	CN-61	CN-SN
19	甘肃	7	620000	CN-62	CN-GS
20	青海	7	630000	CN-63	CN-QH
21	宁夏	7	640000	CN-64	CN-NX
22	新疆	7	650000	CN-65	CN-XJ
23	河南	4	410000	CN-41	CN-HA
24	湖北	4	420000	CN-42	CN-HB
25	湖南	4	430000	CN-43	CN-HN
26	广东	5	440000	CN-44	CN-GD
27	广西	5	450000	CN-45	CN-GX
28	海南	5	460000	CN-46	CN-HI
29	香港	5	810000	CN-91	CN-HK
30	澳门	5	820000	CN-92	CN-MO
31	四川	6	510000	CN-51	CN-SC
32	贵州	6	520000	CN-52	CN-GZ
33	云南	6	530000	CN-53	CN-YN
13	重庆	6	500000	CN-50	CN-CQ
34	西藏	6	540000	CN-54	CN-XZ
```

### 4.2.2 MySQLReader之QuerySQLMode

1）编写配置文件

（1）修改配置文件 base_province.json

```shell
vim /opt/module/datax/job/base_province.json
```

（2）配置文件内容如下

```json title="base_province.json"
{
    "job": {
        "content": [
            {
                "reader": {
                    "name": "mysqlreader",
                    "parameter": {
                        "connection": [
                            {
                                "jdbcUrl": [
                                    "jdbc:mysql://hadoop102:3306/gmall"
                                ],
                                "querySql": [
                                    "select id,name,region_id,area_code,iso_code,iso_3166_2 from base_province where id>=3"
                                ]
                            }
                        ],
                        "password": "000000",
                        "username": "root"
                    }
                },
                "writer": {
                    "name": "hdfswriter",
                    "parameter": {
                        "column": [
                            {
                                "name": "id",
                                "type": "bigint"
                            },
                            {
                                "name": "name",
                                "type": "string"
                            },
                            {
                                "name": "region_id",
                                "type": "string"
                            },
                            {
                                "name": "area_code",
                                "type": "string"
                            },
                            {
                                "name": "iso_code",
                                "type": "string"
                            },
                            {
                                "name": "iso_3166_2",
                                "type": "string"
                            }
                        ],
                        "compress": "gzip",
                        "defaultFS": "hdfs://hadoop102:8020",
                        "fieldDelimiter": "\t",
                        "fileName": "base_province",
                        "fileType": "text",
                        "path": "/base_province",
                        "writeMode": "append"
                    }
                }
            }
        ],
        "setting": {
            "speed": {
                "channel": 1
            }
        }
    }
}
```

2）配置文件说明

（1）Reader 参数说明

![image-20230306111313008](https://cos.gump.cloud/uPic/image-20230306111313008.png)

3）提交任务

（1）清空历史数据

```shell
hadoop fs -rm -r -f /base_province/*
```

（2）进入 DataX 根目录

```shell
cd /opt/module/datax 
```

（3）执行如下命令

```shell
python bin/datax.py job/base_province.json
```

4）查看结果

（1）DataX 打印日志

```shell
2021-10-13 11:13:14.930 [job-0] INFO  JobContainer - 
任务启动时刻                    : 2021-10-13 11:13:03
任务结束时刻                    : 2021-10-13 11:13:14
任务总计耗时                    :                 11s
任务平均流量                    :               66B/s
记录写入速度                    :              3rec/s
读出记录总数                    :                  32
读写失败总数                    :                   0
```

（2）查看 HDFS 文件

```shell
hadoop fs -cat /base_province/* | zcat

3	山西	1	140000	CN-14	CN-SX
4	内蒙古	1	150000	CN-15	CN-NM
5	河北	1	130000	CN-13	CN-HE
6	上海	2	310000	CN-31	CN-SH
7	江苏	2	320000	CN-32	CN-JS
8	浙江	2	330000	CN-33	CN-ZJ
9	安徽	2	340000	CN-34	CN-AH
10	福建	2	350000	CN-35	CN-FJ
11	江西	2	360000	CN-36	CN-JX
12	山东	2	370000	CN-37	CN-SD
14	台湾	2	710000	CN-71	CN-TW
15	黑龙江	3	230000	CN-23	CN-HL
16	吉林	3	220000	CN-22	CN-JL
17	辽宁	3	210000	CN-21	CN-LN
18	陕西	7	610000	CN-61	CN-SN
19	甘肃	7	620000	CN-62	CN-GS
20	青海	7	630000	CN-63	CN-QH
21	宁夏	7	640000	CN-64	CN-NX
22	新疆	7	650000	CN-65	CN-XJ
23	河南	4	410000	CN-41	CN-HA
24	湖北	4	420000	CN-42	CN-HB
25	湖南	4	430000	CN-43	CN-HN
26	广东	5	440000	CN-44	CN-GD
27	广西	5	450000	CN-45	CN-GX
28	海南	5	460000	CN-46	CN-HI
29	香港	5	810000	CN-91	CN-HK
30	澳门	5	820000	CN-92	CN-MO
31	四川	6	510000	CN-51	CN-SC
32	贵州	6	520000	CN-52	CN-GZ
33	云南	6	530000	CN-53	CN-YN
13	重庆	6	500000	CN-50	CN-CQ
34	西藏	6	540000	CN-54	CN-XZ
```

### 4.2.3 DataX传参

通常情况下，离线数据同步任务需要每日定时重复执行，故 HDFS 上的目标路径通常会包含一层日期，以对每日同步的数据加以区分，也就是说每日同步数据的目标路径不是固定不变的，因此 DataX 配置文件中 HDFS Writer 的 path 参数的值应该是动态的。为实现这一效果，就需要使用 DataX 传参的功能。

DataX 传参的用法如下，在 JSON 配置文件中使用 `${param}` 引用参数，在提交任务时使用 `-p"-Dparam=value"` 传入参数值，具体示例如下。

1）编写配置文件

（1）修改配置文件 base_province.json

```shell
vim /opt/module/datax/job/base_province.json
```

（2）配置文件内容如下

```json title="base_province.json"
{
    "job": {
        "content": [
            {
                "reader": {
                    "name": "mysqlreader",
                    "parameter": {
                        "connection": [
                            {
                                "jdbcUrl": [
                                    "jdbc:mysql://hadoop102:3306/gmall"
                                ],
                                "querySql": [
                                    "select id,name,region_id,area_code,iso_code,iso_3166_2 from base_province where id>=3"
                                ]
                            }
                        ],
                        "password": "000000",
                        "username": "root"
                    }
                },
                "writer": {
                    "name": "hdfswriter",
                    "parameter": {
                        "column": [
                            {
                                "name": "id",
                                "type": "bigint"
                            },
                            {
                                "name": "name",
                                "type": "string"
                            },
                            {
                                "name": "region_id",
                                "type": "string"
                            },
                            {
                                "name": "area_code",
                                "type": "string"
                            },
                            {
                                "name": "iso_code",
                                "type": "string"
                            },
                            {
                                "name": "iso_3166_2",
                                "type": "string"
                            }
                        ],
                        "compress": "gzip",
                        "defaultFS": "hdfs://hadoop102:8020",
                        "fieldDelimiter": "\t",
                        "fileName": "base_province",
                        "fileType": "text",
                        "path": "/base_province/${dt}",
                        "writeMode": "append"
                    }
                }
            }
        ],
        "setting": {
            "speed": {
                "channel": 1
            }
        }
    }
}
```

2）提交任务

（1）创建目标路径

```shell
hadoop fs -mkdir /base_province/2020-06-14
```

（2）进入 DataX 根目录

```shell
cd /opt/module/datax 
```

（3）执行如下命令

```shell
python bin/datax.py -p"-Ddt=2020-06-14" job/base_province.json
```

3）查看结果

```shell
hadoop fs -ls /base_province

Found 2 items
drwxr-xr-x   - atguigu supergroup          0 2021-10-15 21:41 /base_province/2020-06-14
```

## 4.3 同步HDFS数据到MySQL案例

案例要求：同步 HDFS 上的 /base_province 目录下的数据到 MySQL gmall  数据库下的 test_province 表。

需求分析：要实现该功能，需选用 HDFSReader 和 MySQLWriter。

1）编写配置文件

（1）创建配置文件 test_province.json

```shell
vim /opt/module/datax/job/base_province.json
```

（2）配置文件内容如下

```json title="base_province.json"
{
    "job": {
        "content": [
            {
                "reader": {
                    "name": "hdfsreader",
                    "parameter": {
                        "defaultFS": "hdfs://hadoop102:8020",
                        "path": "/base_province",
                        "column": [
                            "*"
                        ],
                        "fileType": "text",
                        "compress": "gzip",
                        "encoding": "UTF-8",
                        "nullFormat": "\\N",
                        "fieldDelimiter": "\t",
                    }
                },
                "writer": {
                    "name": "mysqlwriter",
                    "parameter": {
                        "username": "root",
                        "password": "000000",
                        "connection": [
                            {
                                "table": [
                                    "test_province"
                                ],
                                "jdbcUrl": "jdbc:mysql://hadoop102:3306/gmall?useUnicode=true&characterEncoding=utf-8"
                            }
                        ],
                        "column": [
                            "id",
                            "name",
                            "region_id",
                            "area_code",
                            "iso_code",
                            "iso_3166_2"
                        ],
                        "writeMode": "replace"
                    }
                }
            }
        ],
        "setting": {
            "speed": {
                "channel": 1
            }
        }
    }
}
```

2）配置文件说明

（1）Reader 参数说明

![image-20230306112236978](https://cos.gump.cloud/uPic/image-20230306112236978.png)

（2）Writer 参数说明

![image-20230306112255339](https://cos.gump.cloud/uPic/image-20230306112255339.png)

3）提交任务

（1）在 MySQL 中创建 gmall.test_province 表

```sql
DROP TABLE IF EXISTS `test_province`;
CREATE TABLE `test_province`  (
  `id` bigint(20) NOT NULL,
  `name` varchar(20) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  `region_id` varchar(20) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  `area_code` varchar(20) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  `iso_code` varchar(20) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  `iso_3166_2` varchar(20) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;
```

（2）进入 DataX 根目录

```shell
cd /opt/module/datax
```

（3）执行如下命令

```shell
python bin/datax.py job/test_province.json 
```

4）查看结果

（1）DataX 打印日志

```shell
2021-10-13 15:21:35.006 [job-0] INFO  JobContainer - 
任务启动时刻                    : 2021-10-13 15:21:23
任务结束时刻                    : 2021-10-13 15:21:35
任务总计耗时                    :                 11s
任务平均流量                    :               70B/s
记录写入速度                    :              3rec/s
读出记录总数                    :                  34
读写失败总数                    :                   0
```

（2）查看 MySQL 目标表数据

![image-20230306112413333](https://cos.gump.cloud/uPic/image-20230306112413333.png)
