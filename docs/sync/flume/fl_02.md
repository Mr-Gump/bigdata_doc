# 第2章 Flume入门

## 2.1 Flume安装部署

### 2.1.1 安装地址

（1）[:link:Flume 官网地址](http://flume.apache.org/)

（2）[:link:文档查看地址](http://flume.apache.org/FlumeUserGuide.html)

（3）[:link:下载地址](http://archive.apache.org/dist/flume/)

### 2.1.2 安装部署

（1）将 apache-flume-1.9.0-bin.tar.gz 上传到 linux 的 /opt/software 目录下

（2）解压 apache-flume-1.9.0-bin.tar.gz 到 /opt/module/ 目录下

```shell
tar -zxvf /opt/software/apache-flume-1.9.0-bin.tar.gz -C /opt/module/
```

（3）修改 apache-flume-1.9.0-bin 的名称为 flume

```shell
mv /opt/module/apache-flume-1.9.0-bin /opt/module/flume
```

（4）将 lib 文件夹下的 guava-11.0.2.jar 删除以兼容 Hadoop 3.1.3

```shell
rm /opt/module/flume/lib/guava-11.0.2.jar
```

（5）修改 conf 下的 log4j.properties 确定日志打印的位置

```properties title="log4j.properties"
#console表示同时将日志输出到控制台
flume.root.logger=INFO,LOGFILE,console
#固定日志输出的位置
flume.log.dir=/opt/module/flume/logs
#日志文件的名称
flume.log.file=flume.log
```

## 2.2 Flume入门案例

### 2.2.1 监控端口数据官方案例

**1）案例需求：**

使用 Flume 监听一个端口，收集该端口数据，并打印到控制台。 

**2）实现步骤：**

（1）安装 netcat 工具

```shell
sudo yum install -y nc
```

（2）判断 44444 端口是否被占用

```shell
sudo netstat -nlp | grep 44444
```

（3）在 conf 文件夹下创建 Flume Agent 配置文件 nc-flume-log.conf

```conf title="nc-flume-log.conf"
添加内容如下：
# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444

# Describe the sink
a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

!!! info "注"

    配置文件来源于[:link:官方手册](http://flume.apache.org/FlumeUserGuide.html)

（4）先开启 flume 监听端口

第一种写法：

```shell
bin/flume-ng agent --conf conf/ --name a1 --conf-file conf/nc-flume-log.conf -Dflume.root.logger=INFO,console
```

第二种写法：

```shell
bin/flume-ng agent -c conf/ -n a1 -f conf/nc-flume-log.conf -Dflume.root.logger=INFO,console
```

参数说明：

- --conf / -c：表示配置文件存储在 conf/ 目录
- --name / -n：表示给 agent 起名为 a1
- --conf-file / -f：flume 本次启动读取的配置文件是在 conf 文件夹下的 nc-flume-log.conf 文件。
- -Dflume.root.logger=INFO,console ：-D 表示 flume 运行时动态修改 flume.root.logger 参数属性值，并将控制台日志打印级别设置为 INFO 级别。日志级别包括:log、info、warn、error。日志参数已经在配置文件中修改了，不再需要重复输入。

（5）使用 netcat 工具向本机的 44444 端口发送内容

```shell
nc localhost 44444
```

（6）在 Flume 监听页面观察接收数据情况

```shell
2021-07-15 13:51:00,236 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: { headers:{} body: 31 30                                           10 }
```

（7）event 打印的源码介绍

LoggerSink 的 process 方法：

```java
if (event != null) {
    if (logger.isInfoEnabled()) {
        logger.info("Event: " + EventHelper.dumpEvent(event, maxBytesToLog));
    }
}
```

dumpEvent 方法返回值：{==buffer 是固定长度的字符串，前端是 16 进制表示的字符的 ASCII 值==}。

```java
return "{ headers:" + event.getHeaders() + " body:" + buffer + " }";
```

### 2.2.2 实时监控目录下的多个追加文件

Taildir Source 适合用于监听{==多个实时追加的文件==}，并且能够实现{==断点续传==}。

**1）案例需求:使用 Flume 监听整个目录的实时追加文件，并上传至 HDFS** 

**2）需求分析**

![image-20230303231530345](https://cos.gump.cloud/uPic/image-20230303231530345.png)

**3）实现步骤**

（1）在 conf 目录下创建配置文件 taildir-flume-hdfs.conf

①创建一个文件

```shell
vim taildir-flume-hdfs.conf
```

②添加如下内容

```conf title="taildir-flume-hdfs.conf"
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = TAILDIR
a1.sources.r1.filegroups = f1 f2
# 必须精确到文件，可以写匹配表达式匹配多个文件
a1.sources.r1.filegroups.f1 = /opt/module/flume/files1/.*file.*
a1.sources.r1.filegroups.f2 = /opt/module/flume/files2/.*log.*
# 实现断点续传的文件存放位置 不改有默认位置也能实现断点续传
a1.sources.r1.positionFile = /opt/module/flume/taildir_position.json


# Describe the sink
a1.sinks.k1.type = hdfs
# 地址值可以填写hdfs://hadoop102:8020也可以省略,flume会自动读取hadoop配置文件信息获取地址
a1.sinks.k1.hdfs.path = hdfs://hadoop102:8020/flume/%Y%m%d/%H
#上传文件的前缀
a1.sinks.k1.hdfs.filePrefix = log-

#是否使用本地时间戳
a1.sinks.k1.hdfs.useLocalTimeStamp = true

#设置文件类型 分为二进制文件SequenceFile和文本文件DataStream(不能压缩) 和CompressedStream(可以压缩)
a1.sinks.k1.hdfs.fileType = DataStream

#多久生成一个新的文件
a1.sinks.k1.hdfs.rollInterval = 30
#设置每个文件的滚动大小大概是128M
a1.sinks.k1.hdfs.rollSize = 134217700
#文件的滚动与Event数量无关
a1.sinks.k1.hdfs.rollCount = 0 

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

（2）向 files 文件夹中追加内容

在 /opt/module/flume 目录下创建 files1 文件夹

```shell
mkdir files1
mkdir files2
```

（3）启动监控文件夹命令

```shell
bin/flume-ng agent --conf conf/ --name a1 --conf-file conf/taildir-flume-hdfs.conf
```

向 upload 文件夹中添加文件

```shell
echo hello >> file1.txt
echo atguigu >> file2.txt
```

（4）查看 HDFS 上的数据

**Taildir 说明：**

Taildir Source 维护了一个 json 格式的 position File，其会定期的往 position File 中更新每个文件读取到的最新的位置，因此能够实现断点续传。Position File 的格式如下：

```shell
{"inode":2496272,"pos":12,"file":"/opt/module/flume/files1/file1.txt"}
{"inode":2496275,"pos":12,"file":"/opt/module/flume/files1/file2.txt"}
```

!!! info "注"

    Linux 中储存文件元数据的区域就叫做 inode，每个 inode 都有一个号码，操作系统用 inode 号码来识别不同的文件，Unix/Linux 系统内部不使用文件名，而使用 inode 号码来识别文件。TailDir source 使用 inode 和文件的全路径一起识别同一个文件，所以修改文件名之后如果表达式也能够匹配上，会再重新读取一份文件的数据。
