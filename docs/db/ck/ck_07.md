# 第7章 分片集群 

副本虽然能够提高数据的可用性，降低丢失风险，但是每台服务器实际上必须容纳全量数据，对数据的横向扩容没有解决。

要解决数据水平切分的问题，需要引入分片的概念。通过分片把一份完整的数据进行切分，不同的分片分布到不同的节点上，再通过 Distributed 表引擎把数据拼接起来一同使用。

Distributed 表引擎本身不存储数据，有点类似于 MyCat 之于 MySql，成为一种中间件，通过分布式逻辑表来写入、分发、路由来操作多台节点不同分片的分布式数据。

==注意：ClickHouse的集群是表级别的，实际企业中，大部分做了高可用，但是没有用分片，避免降低查询性能以及操作集群的复杂性。==

## 7.1 集群写入流程（3分片2副本共6个节点）

![image-20230128202703554](https://cos.gump.cloud/uPic/image-20230128202703554.png)

## 7.2 集群读取流程（3分片2副本共6个节点）

![image-20230128202720708](https://cos.gump.cloud/uPic/image-20230128202720708.png)

### 7.3 3分片2副本共6个节点集群配置（供参考）

1. 在 /etc/clickhouse-server/config.d/metrika.xml文件中配置

  

  配置的位置还是在之前的/etc/clickhouse-server/config.d/metrika.xml，内容如下

  ```xml
  <yandex>
  	<clickhouse_remote_servers>
  		<gmall_cluster> <!-- 集群名称--> 
  			<shard>  <!--集群的第一个分片-->
  				<internal_replication>true</internal_replication>
  				<!--该分片的第一个副本-->
  			    <replica>    
  			        <host>hadoop102</host>
  			        <port>9000</port>
  			     </replica>
  			     <!--该分片的第二个副本-->
  			     <replica> 
  			        <host>hadoop103</host>
  			        <port>9000</port>
  			     </replica>
  			</shard>
  
  			  <shard>  <!--集群的第二个分片-->
  			     <internal_replication>true</internal_replication>
  			     <replica>    <!--该分片的第一个副本-->
  			        <host>hadoop104</host>
  			     	<port>9000</port>
  			     </replica>
  			     <replica>    <!--该分片的第二个副本-->
  			        <host>hadoop105</host>
  			        <port>9000</port>
  			     </replica>
  			  </shard>
  
  			  <shard>  <!--集群的第三个分片-->
  			     <internal_replication>true</internal_replication>
  			     <replica>     <!--该分片的第一个副本-->
  			        <host>hadoop106</host>
  			        <port>9000</port>
  			     </replica>
  			     <replica>    <!--该分片的第二个副本-->
  			        <host>hadoop107</host>
  			        <port>9000</port>
  			     </replica>
  			  </shard>
  		</gmall_cluster>
  	</clickhouse_remote_servers>
  </yandex>
  ```

## 7.4 配置三节点版本集群及副本

### 7.4.1 集群及副本规划（2个分片，只有第一个分片有副本）

![image-20230128202948509](https://cos.gump.cloud/uPic/image-20230128202948509.png)

=== "hadoop102"
    ```xml
    <macros>
    <shard>01</shard> 
    <replica>rep_1_1</replica>
    </macros>
    ```
=== "hadoop103"
    ```xml
    <macros>
    <shard>01</shard> 
    <replica>rep_1_2</replica>
    </macros>
    ```
=== "hadoop104"
    ```xml
    <macros>
    <shard>02</shard> 
    <replica>rep_2_1</replica>
    </macros>
    ```
    
### 7.4.2 配置步骤
1）在 hadoop102 的 /etc/clickhouse-server/config.d 目录下创建 metrika-shard.xml 文件
```xml title="metrika-shard.xml"
<?xml version="1.0"?>
<yandex>
	<clickhouse_remote_servers>
		<gmall_cluster> <!-- 集群名称--> 
			<shard>         <!--集群的第一个分片-->
			<internal_replication>true</internal_replication>
			    <replica>    <!--该分片的第一个副本-->
			        <host>hadoop102</host>
			        <port>9000</port>
			    </replica>
			    <replica>    <!--该分片的第二个副本-->
			        <host>hadoop103</host>
			        <port>9000</port>
			    </replica>
			</shard>

			<shard>  <!--集群的第二个分片-->
			    <internal_replication>true</internal_replication>
			    <replica>    <!--该分片的第一个副本-->
			        <host>hadoop104</host>
			        <port>9000</port>
			    </replica>
			</shard>
		</gmall_cluster>
	</clickhouse_remote_servers>

	<zookeeper-servers>
		<node index="1">
			<host>hadoop102</host>
		 	<port>2181</port>
		</node>
		<node index="2">
		 	<host>hadoop103</host>
		  	<port>2181</port>
		</node>
		<node index="3">
		  	<host>hadoop104</host>
		  	<port>2181</port>
		</node>
	</zookeeper-servers>

	<macros>
		<shard>01</shard>   <!--不同机器放的分片数不一样-->
		<replica>rep_1_1</replica>  <!--不同机器放的副本数不一样-->
	</macros>
</yandex>
```

2）将 hadoop102 的 metrika-shard.xml 同步到 103 和 104
```bash
sudo /home/atguigu/bin/xsync /etc/clickhouse-server/config.d/metrika-shard.xml
```

3）修改 103,104 中 metrika-shard.xml 宏的配置

=== "hadoop103"
    ```xml
    <macros>
    <shard>01</shard> 
    <replica>rep_1_2</replica>
    </macros>
    ```
=== "hadoop104"
    ```xml
    <macros>
    <shard>02</shard> 
    <replica>rep_2_1</replica>
    </macros>
    ```
    
4）在 hadoop102 上修改 /etc/clickhouse-server/config.xml
```xml title="/etc/clickhouse-server/config.xml"
<include_from>/etc/clickhouse-server/config.d/metrika-shard.xml</include_from>
```

5）同步 /etc/clickhouse-server/config.xml 到 103 和 104
```bash
sudo /home/atguigu/bin/xsync /etc/clickhouse-server/config.xml
```

6）重启三台服务器上的 ClickHouse 服务
```bash
sudo systemctl stop clickhouse-server
sudo systemctl start clickhouse-server
```

### 7.4.3 测试
1）在 hadoop102 上执行建表语句
<div class="termy">
```console
$ CREATE TABLE st_order_mt ON CLUSTER gmall_cluster
$ (
$     `id` UInt32, 
$     `sku_id` String, 
$     `total_amount` Decimal(16, 2), 
$     `create_time` Datetime
$ )
$ ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/st_order_mt', '{replica}')
$ PARTITION BY toYYYYMMDD(create_time)
$ PRIMARY KEY id
$ ORDER BY (id, sku_id)

┌─host──────┬─port─┬─status─┬─error─┬─num_hosts_remaining─┬─num_hosts_active─┐
│ hadoop102 │ 9000 │      0 │       │                   2 │                0 │
│ hadoop104 │ 9000 │      0 │       │                   1 │                0 │
│ hadoop103 │ 9000 │      0 │       │                   0 │                0 │
└───────────┴──────┴────────┴───────┴─────────────────────┴──────────────────┘

3 rows in set. Elapsed: 0.303 sec.
```
</div>

2）查看 hadoop103 和 hadoop104 表是否创建成功

=== "hadoop103"
    <div class="termy">
    ```console
    $ show tables; 

    SHOW TABLES

    ┌─name────────┐
    │ st_order_mt │
    └─────────────┘

    2 rows in set. Elapsed: 0.006 sec.
    ```
    </div>

=== "hadoop104"
    <div class="termy">
    ```console
    $ show tables; 

    SHOW TABLES

    ┌─name────────┐
    │ st_order_mt │
    └─────────────┘

    2 rows in set. Elapsed: 0.006 sec.
    ```
    </div>
    
3）在 hadoop102 上创建 Distribute 分布式表
<div class="termy">
```concole
$ CREATE TABLE st_order_mt_all ON CLUSTER gmall_cluster
$ (
$     `id` UInt32, 
$     `sku_id` String, 
$     `total_amount` Decimal(16, 2), 
$     `create_time` Datetime
$ )
$ ENGINE = Distributed(gmall_cluster, default, st_order_mt, hiveHash(sku_id))

┌─host──────┬─port─┬─status─┬─error─┬─num_hosts_remaining─┬─num_hosts_active─┐
│ hadoop102 │ 9000 │      0 │       │                   2 │                0 │
│ hadoop104 │ 9000 │      0 │       │                   1 │                0 │
│ hadoop103 │ 9000 │      0 │       │                   0 │                0 │
└───────────┴──────┴────────┴───────┴─────────────────────┴──────────────────┘

3 rows in set. Elapsed: 0.158 sec.
```
</div>

参数含义

- Distributed(集群名称，库名，本地表名，分片键)
- 分片键必须是整型数字，所以用hiveHash函数转换，也可以rand()

4）在 hadoop102 上插入测试数据
```sql
insert into  st_order_mt_all values
(201,'sku_001',1000.00,'2020-06-01 12:00:00') ,
(202,'sku_002',2000.00,'2020-06-01 12:00:00'),
(203,'sku_004',2500.00,'2020-06-01 12:00:00'),
(204,'sku_002',2000.00,'2020-06-01 12:00:00'),
(205,'sku_003',600.00,'2020-06-02 12:00:00');
```

5）通过查询分布式表和本地表观察输出结果

分布式表

```sql SELECT *  FROM st_order_mt_all;```

本地表

```sql select * from st_order_mt;```

观察数据的分布

=== "st_order_mt_all"
    ```sql
    ┌──id─┬─sku_id──┬─total_amount─┬─────────create_time─┐
    │ 202 │ sku_002 │      2000.00 │ 2020-06-01 12:00:00 │
    │ 203 │ sku_004 │      2500.00 │ 2020-06-01 12:00:00 │
    │ 204 │ sku_002 │      2000.00 │ 2020-06-01 12:00:00 │
    └─────┴─────────┴──────────────┴─────────────────────┘
    ┌──id─┬─sku_id──┬─total_amount─┬─────────create_time─┐
    │ 201 │ sku_001 │      1000.00 │ 2020-06-01 12:00:00 │
    └─────┴─────────┴──────────────┴─────────────────────┘
    ┌──id─┬─sku_id──┬─total_amount─┬─────────create_time─┐
    │ 205 │ sku_003 │       600.00 │ 2020-06-02 12:00:00 │
    └─────┴─────────┴──────────────┴─────────────────────┘
    ```

=== "hadoop102:st_order_mt"
    ```sql
    ┌──id─┬─sku_id──┬─total_amount─┬─────────create_time─┐
    │ 202 │ sku_002 │      2000.00 │ 2020-06-01 12:00:00 │
    │ 203 │ sku_004 │      2500.00 │ 2020-06-01 12:00:00 │
    │ 204 │ sku_002 │      2000.00 │ 2020-06-01 12:00:00 │
    └─────┴─────────┴──────────────┴─────────────────────┘
    ```

=== "hadoop103:st_order_mt"
    ```sql
    ┌──id─┬─sku_id──┬─total_amount─┬─────────create_time─┐
    │ 202 │ sku_002 │      2000.00 │ 2020-06-01 12:00:00 │
    │ 203 │ sku_004 │      2500.00 │ 2020-06-01 12:00:00 │
    │ 204 │ sku_002 │      2000.00 │ 2020-06-01 12:00:00 │
    └─────┴─────────┴──────────────┴─────────────────────┘
    ```

=== "hadoop104:st_order_mt"
    ```sql
    ┌──id─┬─sku_id──┬─total_amount─┬─────────create_time─┐
    │ 205 │ sku_003 │       600.00 │ 2020-06-02 12:00:00 │
    └─────┴─────────┴──────────────┴─────────────────────┘
    ┌──id─┬─sku_id──┬─total_amount─┬─────────create_time─┐
    │ 201 │ sku_001 │      1000.00 │ 2020-06-01 12:00:00 │
    └─────┴─────────┴──────────────┴─────────────────────┘
    ```