# 第3章 DStream转换

DStream 上的操作与 RDD 的类似，分为{++转换++}和{++输出++}两种，此外转换操作中还有一些比较特殊的原语，如：transform() 以及各种 Window 相关的原语。

## 3.1 无状态转化操作

无状态转化操作：就是把 RDD 转化操作应用到 DStream 每个批次上，{==每个批次相互独立，自己算自己的==}。

### 3.1.1 常规无状态转化操作

DStream 的部分无状态转化操作列在了下表中，都是 DStream 自己的 API。

!!! info "注意"

    只有 `JavaPairDStream<Key, Value>` 才能使用 `xxxByKey()` 类型的转换算子。

| 函数名称      | 目的                                                         | 函数类型                     |
| ------------- | ------------------------------------------------------------ | ---------------------------- |
| map()         | 对 DStream 中的每个元素应用给定函数，返回由各元素输出的元素组成的 DStream。 | Function<in, out>            |
| flatMap()     | 对 DStream 中的每个元素应用给定函数，返回由各元素输出的迭代器组成的 DStream。 | FlatMapFunction<in, out>     |
| filter()      | 返回由给定 DStream 中通过筛选的元素组成的 DStream            | Function<in, Boolean>        |
| mapToPair()   | 改变 DStream 的分区数                                        | PairFunction<in, key, value> |
| reduceByKey() | 将每个批次中键相同的记录规约。                               | Function2<in, in, in>        |
| groupByKey()  | 将每个批次中的记录根据键分组。                               | ds.groupByKey()              |

需要记住的是，尽管这些函数看起来像作用在整个流上一样，但事实上每个 DStream 在内部是由许多 RDD {==批次==}组成，且{==无状态转化操作==}是分别应用到{==每个 RDD (一个批次的数据)==}上的。

## 3.2 窗口操作

### 3.2.1 WindowOperations

Window Operations 可以设置窗口的大小和滑动窗口的间隔来动态的获取当前 Streaming 的允许状态。所有基于窗口的操作都需要两个参数，分别为窗口时长以及滑动步长。

- 窗口时长：计算内容的时间范围
- 滑动步长：隔多久触发一次计算

!!! info "注意"

    这两者都必须为采集批次大小的整数倍。

如下图所示 WordCount 案例：窗口大小为批次的 2 倍，滑动步等于批次大小。

![image-20230210155220946](https://cos.gump.cloud/uPic/image-20230210155220946.png)

### 3.2.2 Window

1）基本语法：

`window(windowLength, slideInterval)` 基于对源 DStream 窗口的批次进行计算返回一个新的 DStream。

2）需求：

统计 WordCount：3 秒一个批次，窗口 12 秒，滑步 6 秒。

3）代码编写：

```java
package com.atguigu;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaInputDStream;
import org.apache.spark.streaming.api.java.JavaPairDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.kafka010.ConsumerStrategies;
import org.apache.spark.streaming.kafka010.KafkaUtils;
import org.apache.spark.streaming.kafka010.LocationStrategies;
import scala.Tuple2;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.Iterator;

public class Test02_Window {
    public static void main(String[] args) throws InterruptedException {
        // 创建流环境
        JavaStreamingContext javaStreamingContext = new JavaStreamingContext("local[*]", "window", Duration.apply(3000));
        // 创建配置参数
        HashMap<String, Object> map = new HashMap<>();
        map.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,"hadoop102:9092,hadoop103:9092,hadoop104:9092");
        map.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");
        map.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");
        map.put(ConsumerConfig.GROUP_ID_CONFIG,"atguigu");
        map.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,"latest");

        // 需要消费的主题
        ArrayList<String> strings = new ArrayList<>();
        strings.add("topicA");

        JavaInputDStream<ConsumerRecord<String, String>> directStream = KafkaUtils.createDirectStream(javaStreamingContext, LocationStrategies.PreferBrokers(), ConsumerStrategies.<String, String>Subscribe(strings,map));

        JavaDStream<String> stringJavaDStream = directStream.flatMap(new FlatMapFunction<ConsumerRecord<String, String>, String>() {
            @Override
            public Iterator<String> call(ConsumerRecord<String, String> stringStringConsumerRecord) throws Exception {
                String[] split = stringStringConsumerRecord.value().split(" ");
                return Arrays.asList(split).iterator();
            }
        });

        JavaPairDStream<String, Integer> javaPairDStream = stringJavaDStream.mapToPair(new PairFunction<String, String, Integer>() {
            @Override
            public Tuple2<String, Integer> call(String s) throws Exception {
                return new Tuple2<>(s, 1);
            }
        });

        JavaPairDStream<String, Integer> window = javaPairDStream.window(Duration.apply(12000), Duration.apply(6000));
        window.reduceByKey(new Function2<Integer, Integer, Integer>() {
            @Override
            public Integer call(Integer v1, Integer v2) throws Exception {
                return v1+v2;
            }
        }).print();

        // 执行流的任务
        javaStreamingContext.start();
        javaStreamingContext.awaitTermination();

    }
}
```

4）测试

```shell
kafka-console-producer.sh --broker-list hadoop102:9092 --topic topicA
```

5）如果有多批数据进入窗口，最终也会通过 window 操作变成统一的 RDD 处理。

![image-20230210155409976](https://cos.gump.cloud/uPic/image-20230210155409976.png)

### 3.2.3 reduceByKeyAndWindow

1）基本语法：

`reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])`：当在一个 ( K , V ) 对的 DStream 上调用此函数，会返回一个新 ( K , V ) 对的 DStream，此处通过对滑动窗口中批次数据使用 reduce 函数来整合每个 key 的 value 值。

2）需求：

统计 WordCount：3 秒一个批次，窗口 12 秒，滑步 6 秒。

3）代码编写：

```java
package com.atguigu;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaInputDStream;
import org.apache.spark.streaming.api.java.JavaPairDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.kafka010.ConsumerStrategies;
import org.apache.spark.streaming.kafka010.KafkaUtils;
import org.apache.spark.streaming.kafka010.LocationStrategies;
import scala.Tuple2;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.Iterator;

public class Test03_ReduceByKeyAndWindow {
    public static void main(String[] args) throws InterruptedException {
        // 创建流环境
        JavaStreamingContext javaStreamingContext = new JavaStreamingContext("local[*]", "window", Duration.apply(3000L));
        // 创建配置参数
        HashMap<String, Object> map = new HashMap<>();
        map.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,"hadoop102:9092,hadoop103:9092,hadoop104:9092");
        map.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");
        map.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");
        map.put(ConsumerConfig.GROUP_ID_CONFIG,"atguigu");
        map.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,"latest");

        // 需要消费的主题
        ArrayList<String> strings = new ArrayList<>();
        strings.add("topicA");

        JavaInputDStream<ConsumerRecord<String, String>> directStream = KafkaUtils.createDirectStream(javaStreamingContext, LocationStrategies.PreferBrokers(), ConsumerStrategies.<String, String>Subscribe(strings,map));

        JavaDStream<String> stringJavaDStream = directStream.flatMap(new FlatMapFunction<ConsumerRecord<String, String>, String>() {
            @Override
            public Iterator<String> call(ConsumerRecord<String, String> stringStringConsumerRecord) throws Exception {
                String[] split = stringStringConsumerRecord.value().split(" ");
                return Arrays.asList(split).iterator();
            }
        });

        JavaPairDStream<String, Integer> javaPairDStream = stringJavaDStream.mapToPair(new PairFunction<String, String, Integer>() {
            @Override
            public Tuple2<String, Integer> call(String s) throws Exception {
                return new Tuple2<>(s, 1);
            }
        });

        javaPairDStream.reduceByKeyAndWindow(new Function2<Integer, Integer, Integer>() {
            @Override
            public Integer call(Integer v1, Integer v2) throws Exception {
                return v1 + v2;
            }
        },Duration.apply(12000L),Duration.apply(6000L)).print();

        // 执行流的任务
        javaStreamingContext.start();
        javaStreamingContext.awaitTermination();

    }
}
```

2）测试

```shell
kafka-console-producer.sh --broker-list hadoop102:9092 --topic topicA
```

